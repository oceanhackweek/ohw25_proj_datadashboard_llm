{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6b596b-5e49-4c10-afe9-055357a0f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cf_xarray\n",
    "import  s3fs, gcsfs, fsspec, zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae60393-445d-4f76-b3e2-07e184381d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper functions to normalize coords\n",
    "\n",
    "def _select_variable(ds: xr.Dataset, var: Union[str, Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Pick a variable name from a Dataset.\n",
    "\n",
    "    If var is a string, it's treated as a variable name.\n",
    "    If cf_xarray is available and var is a dict, it's used for CF-aware selection\n",
    "    (e.g., `{\"standard_name\": \"sea_surface_temperature\"}`).\n",
    "    A fallback attempts to match attributes case-insensitively if CF-selection fails.\n",
    "    \"\"\"\n",
    "    if isinstance(var, str):\n",
    "        if var in ds.data_vars:\n",
    "            return var\n",
    "        for k in ds.data_vars:\n",
    "            if k.lower() == var.lower():\n",
    "                return k\n",
    "        raise KeyError(f\"Variable '{var}' not found. Available: {list(ds.data_vars)}\")\n",
    "\n",
    "    if _HAS_CF:\n",
    "        for key in [\"standard_name\", \"long_name\", \"units\"]:\n",
    "            if key in var:\n",
    "                matches = ds.cf.select_variables(**{key: var[key]})\n",
    "                if matches:\n",
    "                    return list(matches)[0]\n",
    "\n",
    "    key_order = [\"standard_name\", \"long_name\", \"units\"]\n",
    "    for candidate in ds.data_vars:\n",
    "        attrs = {k: str(ds[candidate].attrs.get(k, \"\")).lower() for k in key_order}\n",
    "        if any((k in var) and (str(var[k]).lower() == attrs.get(k, \"\")) for k in key_order):\n",
    "            return candidate\n",
    "    raise KeyError(f\"Could not locate variable from hints {var}. Variables: {list(ds.data_vars)}\")\n",
    "\n",
    "\n",
    "def _normalize_coord_names(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Standardize coordinate names to 'latitude', 'longitude', 'time'.\n",
    "    \"\"\"\n",
    "    rename_map = {}\n",
    "    for alias, standard in {\"lat\": \"latitude\", \"lon\": \"longitude\"}.items():\n",
    "        if alias in ds.coords and standard not in ds.coords:\n",
    "            rename_map[alias] = standard\n",
    "    return ds.rename(rename_map) if rename_map else ds\n",
    "\n",
    "\n",
    "def _infer_target_lon_frame(lon_min: float, lon_max: float) -> str:\n",
    "    \"\"\"\n",
    "    Infers whether user-provided longitude bounds are in 0-360 or -180-180 frame.\n",
    "    \"\"\"\n",
    "    return \"0-360\" if (lon_min >= 0 and lon_max <= 360) else \"-180-180\"\n",
    "\n",
    "\n",
    "def _coerce_longitudes(ds: xr.Dataset, target_frame: str, assume_frame: Optional[str] = None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Coerce dataset longitudes to a target frame ('0-360' or '-180-180').\n",
    "    \"\"\"\n",
    "    if \"longitude\" not in ds.coords:\n",
    "        return ds\n",
    "\n",
    "    lon = ds[\"longitude\"].values\n",
    "    if assume_frame:\n",
    "        current = assume_frame\n",
    "    else:\n",
    "        current = \"0-360\" if (np.nanmin(lon) >= 0 and np.nanmax(lon) <= 360) else \"-180-180\"\n",
    "\n",
    "    if current == target_frame:\n",
    "        return ds\n",
    "\n",
    "    if target_frame == \"0-360\":\n",
    "        lon_new = np.mod(lon, 360.0)\n",
    "    else:  # target is -180-180\n",
    "        lon_new = ((lon + 180) % 360) - 180\n",
    "    \n",
    "    ds = ds.assign_coords(longitude=lon_new)\n",
    "    return ds.sortby(\"longitude\")\n",
    "\n",
    "\n",
    "def _ensure_lat_monotonic(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Ensures the latitude coordinate is monotonically increasing.\n",
    "    \"\"\"\n",
    "    if \"latitude\" in ds.coords and ds[\"latitude\"].ndim == 1 and ds[\"latitude\"].values[0] > ds[\"latitude\"].values[-1]:\n",
    "        return ds.sortby(\"latitude\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def _slice_longitude(ds: xr.Dataset, lon_min: float, lon_max: float) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Slice longitude robustly, handling wrap-around for ranges like 350E to 10E.\n",
    "    \"\"\"\n",
    "    if lon_min <= lon_max:\n",
    "        return ds.sel(longitude=slice(lon_min, lon_max))\n",
    "    \n",
    "    lon = ds[\"longitude\"]\n",
    "    part1 = ds.sel(longitude=slice(lon_min, float(lon.max())))\n",
    "    part2 = ds.sel(longitude=slice(float(lon.min()), lon_max))\n",
    "    return xr.concat([part1, part2], dim=\"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ebf53d-efcf-4498-aac2-86dba953ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aws_dataset(\n",
    "    s3_path: str,\n",
    "    variable_of_interest: Union[str, Dict[str, str]],\n",
    "    region_of_interest: Optional[Dict[str, float]] = None,\n",
    "    time_of_interest: Optional[Union[slice, Tuple[str, str]]] = None,\n",
    "    *,\n",
    "    group: Optional[str] = None,\n",
    "    consolidated: Optional[bool] = None,\n",
    "    chunks: Optional[Dict] = None,\n",
    "    assume_lon: Optional[str] = None,  # \"0-360\" or \"-180-180\" if you know...\n",
    "    return_dataset: bool = False,\n",
    "    save_to: Optional[Union[str, pathlib.Path]] = None,\n",
    ") -> Union[xr.DataArray, xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Load and subset a Zarr dataset from a public AWS S3 bucket.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s3_path:\n",
    "        The full S3 path to the Zarr store (e.g., \"s3://era5-pds/zarr/...\").\n",
    "    variable_of_interest:\n",
    "        - Name of the variable in the dataset (e.g., \"sst\", \"tos\", \"t2m\"), OR\n",
    "        - A mapping of CF/long-name hints to try, e.g.:\n",
    "          {\"standard_name\": \"sea_surface_temperature\"}\n",
    "    region_of_interest:\n",
    "        Dict with geographic bounds: {\"lat_min\": -90, \"lat_max\": 90, \"lon_min\": 0, \"lon_max\": 360}.\n",
    "        Longitudes may be 0–360 or −180–180. Function will reconcile.\n",
    "    time_of_interest:\n",
    "        Either a Python slice (e.g., slice(\"1990-01-01\",\"2000-12-31\")) or a 2-tuple of ISO strings.\n",
    "    group:\n",
    "        Zarr group within the store (e.g., \"spatial\" for ERA5).\n",
    "    consolidated:\n",
    "        Whether the Zarr store is consolidated. If None, attempts sensible defaults.\n",
    "    chunks:\n",
    "        Dask chunking dict, e.g., {\"time\": 2400}.\n",
    "    assume_lon:\n",
    "        If set, forces interpretation of dataset longitudes as \"0-360\" or \"-180-180\".\n",
    "    return_dataset:\n",
    "        If True, return the full Dataset. Otherwise return the selected DataArray.\n",
    "    save_to:\n",
    "        Optional path to save the subset as NetCDF.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray or xr.Dataset\n",
    "        The subsetted data.\n",
    "    \"\"\"\n",
    "    # normalize input params\n",
    "    if isinstance(time_of_interest, tuple):\n",
    "        time_of_interest = slice(time_of_interest[0], time_of_interest[1])\n",
    "\n",
    "    region = region_of_interest or {}\n",
    "    lat_min = region.get(\"lat_min\", None)\n",
    "    lat_max = region.get(\"lat_max\", None)\n",
    "    lon_min = region.get(\"lon_min\", None)\n",
    "    lon_max = region.get(\"lon_max\", None)\n",
    "\n",
    "    # config aws\n",
    "    storage_options = {\"anon\": True}\n",
    "    if consolidated is None:\n",
    "        consolidated = False if (group is not None and \"era5\" in s3_path.lower()) else True\n",
    "\n",
    "    # open dataset\n",
    "    ds = xr.open_dataset(\n",
    "        s3_path,\n",
    "        engine=\"zarr\",\n",
    "        chunks=chunks,\n",
    "        consolidated=consolidated,\n",
    "        backend_kwargs={\n",
    "            \"storage_options\": storage_options,\n",
    "            **({\"group\": group} if group else {}),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # select variable (cf-aware if possible)\n",
    "    var = _select_variable(ds, variable_of_interest)\n",
    "\n",
    "    # normalize coordinate names\n",
    "    ds = _normalize_coord_names(ds)\n",
    "\n",
    "    # fix lon to desired slicing frame, if needed \n",
    "    if (lon_min is not None) and (lon_max is not None):\n",
    "        ds = _coerce_longitudes(ds, target_frame=_infer_target_lon_frame(lon_min, lon_max), assume_frame=assume_lon)\n",
    "\n",
    "    # wnsure latitude selection works if lat is descending (ERA5 style)\n",
    "    if (lat_min is not None) and (lat_max is not None):\n",
    "        ds = _ensure_lat_monotonic(ds)\n",
    "\n",
    "    # apply coord selections\n",
    "    sel = ds\n",
    "    if time_of_interest is not None and (\"time\" in sel.dims or \"time\" in sel.coords):\n",
    "        sel = sel.sel(time=time_of_interest)\n",
    "    if (lat_min is not None) and (lat_max is not None) and \"latitude\" in sel.coords:\n",
    "        sel = sel.sel(latitude=slice(min(lat_min, lat_max), max(lat_min, lat_max)))\n",
    "    if (lon_min is not None) and (lon_max is not None) and \"longitude\" in sel.coords:\n",
    "        sel = _slice_longitude(sel, lon_min, lon_max)\n",
    "\n",
    "    # return subset da/ds\n",
    "    out = sel if return_dataset else sel[var]\n",
    "    if save_to is not None:\n",
    "        save_path = pathlib.Path(save_to).expanduser().resolve()\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        out.to_netcdf(save_path)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e2d45-2f97-40d3-9596-634bf031320a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
